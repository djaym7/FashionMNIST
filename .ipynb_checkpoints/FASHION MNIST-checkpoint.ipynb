{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fashion mnist dataset from keras\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "(X_train,y_train),(X_test,y_test) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "[9 0 0 ... 3 0 5]\n",
      "(10000,)\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "#Exploring data shows 60k 28x28 train images and 10,000 test images. Labels are from 0-9 1D\n",
    "print(X_train.shape) \n",
    "#print(X_train[0])\n",
    "print(X_test.shape) \n",
    "print(y_train.shape) \n",
    "print(y_train)\n",
    "print(y_test.shape) \n",
    "print(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing images\n",
    "plt.imshow(X_train[0])\n",
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling data and reshaping\n",
    "#We know that the pixel value here ranges from 0-255, so divide it by 255 and all values will be in range 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test/255.0\n",
    "w,h=28,28\n",
    "X_train = X_train.reshape(X_train.shape[0], w, h, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], w, h, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\nSaveVar(X_train,'X_train')\\nSaveVar(X_test,'X_test')\\nSaveVar(y_train,'y_train')\\nSaveVar(y_test,'y_test')\\n\\nX_train = ResVar('X_train')\\nX_test = ResVar('X_test')\\ny_train = ResVar('y_train')\\ny_test = ResVar('y_test')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving data for resuing it later on \n",
    "import pickle\n",
    "\n",
    "def SaveVar(var,name):\n",
    "    pickle_out = open(name,\"wb\")\n",
    "    pickle.dump(var,pickle_out)\n",
    "    pickle_out.close()\n",
    "#Restoring\n",
    "def ResVar(name):\n",
    "    pickle_in= open(name,\"rb\")\n",
    "    return pickle.load(pickle_in)\n",
    "'''    \n",
    "SaveVar(X_train,'X_train')\n",
    "SaveVar(X_test,'X_test')\n",
    "SaveVar(y_train,'y_train')\n",
    "SaveVar(y_test,'y_test')\n",
    "\n",
    "X_train = ResVar('X_train')\n",
    "X_test = ResVar('X_test')\n",
    "y_train = ResVar('y_train')\n",
    "y_test = ResVar('y_test')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 105s 2ms/step - loss: 0.6058 - acc: 0.8079 - val_loss: 0.3955 - val_acc: 0.8586\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 109s 3ms/step - loss: 0.3756 - acc: 0.8656 - val_loss: 0.3437 - val_acc: 0.8731\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 108s 3ms/step - loss: 0.3343 - acc: 0.8795 - val_loss: 0.4623 - val_acc: 0.8396\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 116s 3ms/step - loss: 0.3075 - acc: 0.8872 - val_loss: 0.5334 - val_acc: 0.8366\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 112s 3ms/step - loss: 0.2911 - acc: 0.8930 - val_loss: 0.3102 - val_acc: 0.8881\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 115s 3ms/step - loss: 0.2753 - acc: 0.8997 - val_loss: 0.7007 - val_acc: 0.8118\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 113s 3ms/step - loss: 0.2638 - acc: 0.9027 - val_loss: 0.4364 - val_acc: 0.8522\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 116s 3ms/step - loss: 0.2489 - acc: 0.9079 - val_loss: 0.7258 - val_acc: 0.8094\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 117s 3ms/step - loss: 0.2369 - acc: 0.9119 - val_loss: 0.3369 - val_acc: 0.8847\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 117s 3ms/step - loss: 0.2234 - acc: 0.9170 - val_loss: 0.3214 - val_acc: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2073bd68978>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tried various layers and activation functions here while lerarning it from the book : leaky relu, he uniform....\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout,Activation,Conv2D,MaxPooling2D,LeakyReLU,BatchNormalization\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "\n",
    "Name = 'FashionMnist CNN64-leakyRELU-BatchNORM-{}'.format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='C:/Users/djaym7/Tensorflow/logs/{}'.format(Name))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=2,input_shape=(28,28,1) )) \n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=2))  \n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64,kernel_size=2))  \n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(256))  \n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=10,validation_split=0.3,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 0.4666 - acc: 0.8380 - val_loss: 0.3802 - val_acc: 0.8699\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 24s 581us/step - loss: 0.3265 - acc: 0.8838 - val_loss: 0.3252 - val_acc: 0.8868\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 25s 605us/step - loss: 0.2920 - acc: 0.8956 - val_loss: 0.3094 - val_acc: 0.8924\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 0.2717 - acc: 0.9032 - val_loss: 0.3032 - val_acc: 0.8961\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 0.2529 - acc: 0.9093 - val_loss: 0.2964 - val_acc: 0.8962\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 0.2391 - acc: 0.9149 - val_loss: 0.3011 - val_acc: 0.8949\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 25s 606us/step - loss: 0.2286 - acc: 0.9186 - val_loss: 0.2902 - val_acc: 0.9017\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 26s 608us/step - loss: 0.2161 - acc: 0.9240 - val_loss: 0.2881 - val_acc: 0.9021\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 26s 611us/step - loss: 0.2066 - acc: 0.9260 - val_loss: 0.2896 - val_acc: 0.9022\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 0.1950 - acc: 0.9296 - val_loss: 0.2961 - val_acc: 0.9004\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 33s 793us/step - loss: 0.5054 - acc: 0.8185 - val_loss: 0.3694 - val_acc: 0.8719\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 35s 823us/step - loss: 0.3407 - acc: 0.8772 - val_loss: 0.3333 - val_acc: 0.8819\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 35s 836us/step - loss: 0.2993 - acc: 0.8935 - val_loss: 0.3146 - val_acc: 0.8890\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 35s 836us/step - loss: 0.2732 - acc: 0.9009 - val_loss: 0.2914 - val_acc: 0.8941\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 35s 836us/step - loss: 0.2498 - acc: 0.9090 - val_loss: 0.2809 - val_acc: 0.8976\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 35s 838us/step - loss: 0.2323 - acc: 0.9156 - val_loss: 0.3163 - val_acc: 0.8907\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 35s 838us/step - loss: 0.2172 - acc: 0.9200 - val_loss: 0.2697 - val_acc: 0.9036\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 36s 867us/step - loss: 0.2010 - acc: 0.9266 - val_loss: 0.2800 - val_acc: 0.9019\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 36s 847us/step - loss: 0.1893 - acc: 0.9297 - val_loss: 0.2609 - val_acc: 0.9078\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 35s 845us/step - loss: 0.1775 - acc: 0.9350 - val_loss: 0.2690 - val_acc: 0.9079\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 39s 927us/step - loss: 0.6331 - acc: 0.7715 - val_loss: 0.4635 - val_acc: 0.8358\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 39s 924us/step - loss: 0.4254 - acc: 0.8457 - val_loss: 0.4179 - val_acc: 0.8456\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 40s 962us/step - loss: 0.3728 - acc: 0.8625 - val_loss: 0.3682 - val_acc: 0.8674\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 41s 968us/step - loss: 0.3395 - acc: 0.8746 - val_loss: 0.3424 - val_acc: 0.8718\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 41s 985us/step - loss: 0.3144 - acc: 0.8838 - val_loss: 0.3407 - val_acc: 0.8753\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 41s 986us/step - loss: 0.2974 - acc: 0.8902 - val_loss: 0.3298 - val_acc: 0.8829\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 41s 986us/step - loss: 0.2837 - acc: 0.8948 - val_loss: 0.3115 - val_acc: 0.8874\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 42s 989us/step - loss: 0.2708 - acc: 0.8993 - val_loss: 0.3117 - val_acc: 0.8883\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 42s 989us/step - loss: 0.2574 - acc: 0.9050 - val_loss: 0.3282 - val_acc: 0.8803\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 42s 991us/step - loss: 0.2451 - acc: 0.9086 - val_loss: 0.3157 - val_acc: 0.8889\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 30s 725us/step - loss: 0.4744 - acc: 0.8338 - val_loss: 0.3762 - val_acc: 0.8679\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 30s 716us/step - loss: 0.3350 - acc: 0.8815 - val_loss: 0.3471 - val_acc: 0.8777\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 31s 728us/step - loss: 0.2994 - acc: 0.8939 - val_loss: 0.3122 - val_acc: 0.8904\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 30s 721us/step - loss: 0.2769 - acc: 0.9013 - val_loss: 0.3129 - val_acc: 0.8922\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 30s 718us/step - loss: 0.2595 - acc: 0.9083 - val_loss: 0.2945 - val_acc: 0.8987\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 30s 717us/step - loss: 0.2454 - acc: 0.9107 - val_loss: 0.2963 - val_acc: 0.8989\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 30s 717us/step - loss: 0.2318 - acc: 0.9175 - val_loss: 0.2896 - val_acc: 0.9002\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 30s 718us/step - loss: 0.2201 - acc: 0.9201 - val_loss: 0.2978 - val_acc: 0.8966\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 30s 718us/step - loss: 0.2120 - acc: 0.9241 - val_loss: 0.2952 - val_acc: 0.8978\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 30s 715us/step - loss: 0.2025 - acc: 0.9273 - val_loss: 0.2937 - val_acc: 0.8988\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.5023 - acc: 0.8219 - val_loss: 0.3675 - val_acc: 0.8669\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.3445 - acc: 0.8774 - val_loss: 0.3320 - val_acc: 0.8839\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.3089 - acc: 0.8888 - val_loss: 0.3382 - val_acc: 0.8762\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.2828 - acc: 0.8976 - val_loss: 0.3020 - val_acc: 0.8923\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.2609 - acc: 0.9059 - val_loss: 0.2794 - val_acc: 0.8997\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.2418 - acc: 0.9118 - val_loss: 0.2681 - val_acc: 0.9040\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 45s 1ms/step - loss: 0.2252 - acc: 0.9169 - val_loss: 0.2808 - val_acc: 0.8995\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 45s 1ms/step - loss: 0.2093 - acc: 0.9236 - val_loss: 0.2794 - val_acc: 0.9016\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 45s 1ms/step - loss: 0.1969 - acc: 0.9282 - val_loss: 0.2634 - val_acc: 0.9084\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 45s 1ms/step - loss: 0.1840 - acc: 0.9335 - val_loss: 0.2722 - val_acc: 0.9080\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.6442 - acc: 0.7639 - val_loss: 0.4737 - val_acc: 0.8274\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.4325 - acc: 0.8419 - val_loss: 0.4281 - val_acc: 0.8427\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.3814 - acc: 0.8611 - val_loss: 0.3937 - val_acc: 0.8546\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.3513 - acc: 0.8724 - val_loss: 0.3546 - val_acc: 0.8702\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.3273 - acc: 0.8796 - val_loss: 0.3511 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.3121 - acc: 0.8852 - val_loss: 0.3662 - val_acc: 0.8663\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.2948 - acc: 0.8910 - val_loss: 0.3202 - val_acc: 0.8837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 48s 1ms/step - loss: 0.2794 - acc: 0.8974 - val_loss: 0.3092 - val_acc: 0.8865\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 48s 1ms/step - loss: 0.2687 - acc: 0.9004 - val_loss: 0.3107 - val_acc: 0.8892\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 48s 1ms/step - loss: 0.2556 - acc: 0.9050 - val_loss: 0.3133 - val_acc: 0.8895\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 29s 681us/step - loss: 0.4639 - acc: 0.8377 - val_loss: 0.3626 - val_acc: 0.8751\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 28s 674us/step - loss: 0.3267 - acc: 0.8824 - val_loss: 0.3290 - val_acc: 0.8840\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 28s 675us/step - loss: 0.2928 - acc: 0.8959 - val_loss: 0.3073 - val_acc: 0.8907\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 28s 674us/step - loss: 0.2706 - acc: 0.9027 - val_loss: 0.2991 - val_acc: 0.8944\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 28s 675us/step - loss: 0.2532 - acc: 0.9110 - val_loss: 0.3049 - val_acc: 0.8936\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 29s 689us/step - loss: 0.2392 - acc: 0.9150 - val_loss: 0.3133 - val_acc: 0.8889\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 28s 676us/step - loss: 0.2283 - acc: 0.9182 - val_loss: 0.2979 - val_acc: 0.8957\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 28s 677us/step - loss: 0.2158 - acc: 0.9237 - val_loss: 0.2832 - val_acc: 0.9027\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 28s 675us/step - loss: 0.2052 - acc: 0.9260 - val_loss: 0.2939 - val_acc: 0.9001\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 28s 675us/step - loss: 0.1980 - acc: 0.9289 - val_loss: 0.3014 - val_acc: 0.8952\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 36s 863us/step - loss: 0.5000 - acc: 0.8208 - val_loss: 0.3801 - val_acc: 0.8647\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 36s 856us/step - loss: 0.3409 - acc: 0.8795 - val_loss: 0.3242 - val_acc: 0.8857\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 36s 856us/step - loss: 0.3052 - acc: 0.8907 - val_loss: 0.3402 - val_acc: 0.8786\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 36s 856us/step - loss: 0.2778 - acc: 0.8996 - val_loss: 0.2876 - val_acc: 0.8977\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 36s 856us/step - loss: 0.2582 - acc: 0.9063 - val_loss: 0.2854 - val_acc: 0.8985\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 36s 857us/step - loss: 0.2380 - acc: 0.9130 - val_loss: 0.2804 - val_acc: 0.9011\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 36s 857us/step - loss: 0.2217 - acc: 0.9196 - val_loss: 0.2649 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 36s 858us/step - loss: 0.2077 - acc: 0.9230 - val_loss: 0.2699 - val_acc: 0.9059\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 36s 858us/step - loss: 0.1936 - acc: 0.9292 - val_loss: 0.2673 - val_acc: 0.9078\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 36s 859us/step - loss: 0.1821 - acc: 0.9347 - val_loss: 0.2708 - val_acc: 0.9066\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 40s 944us/step - loss: 0.6295 - acc: 0.7722 - val_loss: 0.4783 - val_acc: 0.8203\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 39s 933us/step - loss: 0.4272 - acc: 0.8431 - val_loss: 0.4205 - val_acc: 0.8459\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 39s 934us/step - loss: 0.3731 - acc: 0.8657 - val_loss: 0.3707 - val_acc: 0.8631\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 39s 936us/step - loss: 0.3419 - acc: 0.8737 - val_loss: 0.3428 - val_acc: 0.8738\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 39s 934us/step - loss: 0.3179 - acc: 0.8831 - val_loss: 0.3401 - val_acc: 0.8730\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 39s 935us/step - loss: 0.2953 - acc: 0.8910 - val_loss: 0.3238 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 39s 937us/step - loss: 0.2839 - acc: 0.8950 - val_loss: 0.3080 - val_acc: 0.8871\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 39s 937us/step - loss: 0.2693 - acc: 0.9009 - val_loss: 0.3171 - val_acc: 0.8828\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 39s 936us/step - loss: 0.2560 - acc: 0.9062 - val_loss: 0.3032 - val_acc: 0.8905\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 39s 940us/step - loss: 0.2463 - acc: 0.9103 - val_loss: 0.2927 - val_acc: 0.8950\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 34s 811us/step - loss: 0.4720 - acc: 0.8343 - val_loss: 0.3586 - val_acc: 0.8787\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 34s 802us/step - loss: 0.3216 - acc: 0.8857 - val_loss: 0.3238 - val_acc: 0.8850\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 34s 804us/step - loss: 0.2774 - acc: 0.9016 - val_loss: 0.2913 - val_acc: 0.8971\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 34s 806us/step - loss: 0.2479 - acc: 0.9105 - val_loss: 0.2800 - val_acc: 0.8992\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 34s 805us/step - loss: 0.2233 - acc: 0.9187 - val_loss: 0.2926 - val_acc: 0.8967\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 34s 807us/step - loss: 0.2047 - acc: 0.9264 - val_loss: 0.2806 - val_acc: 0.9008\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 34s 810us/step - loss: 0.1872 - acc: 0.9324 - val_loss: 0.2848 - val_acc: 0.8993\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 34s 811us/step - loss: 0.1711 - acc: 0.9375 - val_loss: 0.2773 - val_acc: 0.9077\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 34s 811us/step - loss: 0.1576 - acc: 0.9424 - val_loss: 0.3049 - val_acc: 0.9001\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 34s 812us/step - loss: 0.1429 - acc: 0.9473 - val_loss: 0.2967 - val_acc: 0.9043\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 39s 932us/step - loss: 0.5027 - acc: 0.8211 - val_loss: 0.3755 - val_acc: 0.8655\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 39s 924us/step - loss: 0.3330 - acc: 0.8800 - val_loss: 0.3129 - val_acc: 0.8889\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 39s 925us/step - loss: 0.2916 - acc: 0.8940 - val_loss: 0.2968 - val_acc: 0.8927\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 39s 926us/step - loss: 0.2644 - acc: 0.9025 - val_loss: 0.2808 - val_acc: 0.8994\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 39s 927us/step - loss: 0.2410 - acc: 0.9104 - val_loss: 0.3066 - val_acc: 0.8907\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 39s 927us/step - loss: 0.2208 - acc: 0.9192 - val_loss: 0.2772 - val_acc: 0.8977\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 39s 928us/step - loss: 0.2030 - acc: 0.9257 - val_loss: 0.2538 - val_acc: 0.9085\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 39s 929us/step - loss: 0.1876 - acc: 0.9293 - val_loss: 0.2755 - val_acc: 0.9064\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 39s 931us/step - loss: 0.1706 - acc: 0.9367 - val_loss: 0.2681 - val_acc: 0.9056\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 39s 931us/step - loss: 0.1592 - acc: 0.9410 - val_loss: 0.2614 - val_acc: 0.9072\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 42s 996us/step - loss: 0.6606 - acc: 0.7550 - val_loss: 0.4820 - val_acc: 0.8239\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 41s 988us/step - loss: 0.4348 - acc: 0.8415 - val_loss: 0.4269 - val_acc: 0.8442\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 41s 987us/step - loss: 0.3764 - acc: 0.8615 - val_loss: 0.4281 - val_acc: 0.8366\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 41s 985us/step - loss: 0.3443 - acc: 0.8738 - val_loss: 0.3752 - val_acc: 0.8642\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 41s 981us/step - loss: 0.3205 - acc: 0.8824 - val_loss: 0.3426 - val_acc: 0.8756\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 41s 982us/step - loss: 0.3018 - acc: 0.8901 - val_loss: 0.3471 - val_acc: 0.8736\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 41s 981us/step - loss: 0.2866 - acc: 0.8963 - val_loss: 0.3242 - val_acc: 0.8832\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 41s 982us/step - loss: 0.2711 - acc: 0.8995 - val_loss: 0.3152 - val_acc: 0.8835\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 41s 979us/step - loss: 0.2570 - acc: 0.9056 - val_loss: 0.3448 - val_acc: 0.8764\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 41s 979us/step - loss: 0.2479 - acc: 0.9076 - val_loss: 0.3165 - val_acc: 0.8873\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 39s 922us/step - loss: 0.4388 - acc: 0.8447 - val_loss: 0.3400 - val_acc: 0.8837\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 38s 911us/step - loss: 0.3030 - acc: 0.8907 - val_loss: 0.3078 - val_acc: 0.8900\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 38s 909us/step - loss: 0.2590 - acc: 0.9056 - val_loss: 0.2859 - val_acc: 0.8978\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 38s 908us/step - loss: 0.2244 - acc: 0.9184 - val_loss: 0.2683 - val_acc: 0.9034\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 39s 919us/step - loss: 0.1995 - acc: 0.9272 - val_loss: 0.2593 - val_acc: 0.9086\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 38s 909us/step - loss: 0.1783 - acc: 0.9354 - val_loss: 0.2591 - val_acc: 0.9088\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 38s 906us/step - loss: 0.1590 - acc: 0.9422 - val_loss: 0.2708 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 38s 905us/step - loss: 0.1385 - acc: 0.9506 - val_loss: 0.2849 - val_acc: 0.9081\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 38s 905us/step - loss: 0.1237 - acc: 0.9544 - val_loss: 0.2856 - val_acc: 0.9099\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 38s 905us/step - loss: 0.1079 - acc: 0.9610 - val_loss: 0.2985 - val_acc: 0.9092\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 36s 850us/step - loss: 0.4703 - acc: 0.8286 - val_loss: 0.3642 - val_acc: 0.8645\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 35s 843us/step - loss: 0.3173 - acc: 0.8847 - val_loss: 0.3038 - val_acc: 0.8908\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 36s 854us/step - loss: 0.2736 - acc: 0.8980 - val_loss: 0.2842 - val_acc: 0.8991\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 36s 868us/step - loss: 0.2448 - acc: 0.9093 - val_loss: 0.2720 - val_acc: 0.9021\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 37s 888us/step - loss: 0.2197 - acc: 0.9187 - val_loss: 0.2775 - val_acc: 0.9034\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 37s 890us/step - loss: 0.1969 - acc: 0.9271 - val_loss: 0.2585 - val_acc: 0.9114\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 37s 889us/step - loss: 0.1772 - acc: 0.9339 - val_loss: 0.2606 - val_acc: 0.9084\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 37s 890us/step - loss: 0.1583 - acc: 0.9401 - val_loss: 0.2681 - val_acc: 0.9068\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 37s 892us/step - loss: 0.1399 - acc: 0.9481 - val_loss: 0.2751 - val_acc: 0.9116\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 37s 890us/step - loss: 0.1262 - acc: 0.9535 - val_loss: 0.3102 - val_acc: 0.9014\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 40s 942us/step - loss: 0.6171 - acc: 0.7755 - val_loss: 0.4656 - val_acc: 0.8307\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 39s 931us/step - loss: 0.4151 - acc: 0.8490 - val_loss: 0.4212 - val_acc: 0.8459\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 39s 930us/step - loss: 0.3677 - acc: 0.8651 - val_loss: 0.3622 - val_acc: 0.8672\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 39s 930us/step - loss: 0.3341 - acc: 0.8756 - val_loss: 0.3428 - val_acc: 0.8719\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 39s 933us/step - loss: 0.3143 - acc: 0.8842 - val_loss: 0.3393 - val_acc: 0.8763\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 39s 932us/step - loss: 0.2941 - acc: 0.8900 - val_loss: 0.3208 - val_acc: 0.8821\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 39s 931us/step - loss: 0.2769 - acc: 0.8966 - val_loss: 0.3263 - val_acc: 0.8809\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 39s 933us/step - loss: 0.2636 - acc: 0.9014 - val_loss: 0.3450 - val_acc: 0.8732\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 40s 950us/step - loss: 0.2494 - acc: 0.9071 - val_loss: 0.2977 - val_acc: 0.8929\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 40s 951us/step - loss: 0.2387 - acc: 0.9120 - val_loss: 0.3081 - val_acc: 0.8888\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 52s 1ms/step - loss: 0.4334 - acc: 0.8461 - val_loss: 0.3292 - val_acc: 0.8833\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 52s 1ms/step - loss: 0.2947 - acc: 0.8928 - val_loss: 0.3090 - val_acc: 0.8884\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 52s 1ms/step - loss: 0.2470 - acc: 0.9100 - val_loss: 0.2814 - val_acc: 0.8967\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 52s 1ms/step - loss: 0.2155 - acc: 0.9202 - val_loss: 0.2601 - val_acc: 0.9064\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 52s 1ms/step - loss: 0.1867 - acc: 0.9315 - val_loss: 0.2566 - val_acc: 0.9102\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 52s 1ms/step - loss: 0.1600 - acc: 0.9411 - val_loss: 0.2736 - val_acc: 0.9071\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 53s 1ms/step - loss: 0.1410 - acc: 0.9479 - val_loss: 0.2736 - val_acc: 0.9103\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 54s 1ms/step - loss: 0.1210 - acc: 0.9568 - val_loss: 0.2887 - val_acc: 0.9118\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 51s 1ms/step - loss: 0.1024 - acc: 0.9617 - val_loss: 0.3164 - val_acc: 0.9041\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 54s 1ms/step - loss: 0.0881 - acc: 0.9689 - val_loss: 0.3021 - val_acc: 0.9103\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 38s 896us/step - loss: 0.4559 - acc: 0.8361 - val_loss: 0.3316 - val_acc: 0.8795\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 38s 897us/step - loss: 0.3057 - acc: 0.8888 - val_loss: 0.3059 - val_acc: 0.8892\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 41s 970us/step - loss: 0.2597 - acc: 0.9037 - val_loss: 0.2700 - val_acc: 0.9009\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2277 - acc: 0.9157 - val_loss: 0.2605 - val_acc: 0.9046\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.1976 - acc: 0.9253 - val_loss: 0.2530 - val_acc: 0.9086\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.1728 - acc: 0.9339 - val_loss: 0.2443 - val_acc: 0.9147\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 45s 1ms/step - loss: 0.1494 - acc: 0.9439 - val_loss: 0.2398 - val_acc: 0.9178\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 46s 1ms/step - loss: 0.1287 - acc: 0.9513 - val_loss: 0.2502 - val_acc: 0.9158\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.1092 - acc: 0.9593 - val_loss: 0.3069 - val_acc: 0.9037\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 46s 1ms/step - loss: 0.0934 - acc: 0.9648 - val_loss: 0.3043 - val_acc: 0.9154\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 37s 888us/step - loss: 0.6018 - acc: 0.7808 - val_loss: 0.4684 - val_acc: 0.8269\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 37s 881us/step - loss: 0.4075 - acc: 0.8509 - val_loss: 0.4097 - val_acc: 0.8509\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 38s 910us/step - loss: 0.3507 - acc: 0.8711 - val_loss: 0.3539 - val_acc: 0.8683\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 39s 921us/step - loss: 0.3162 - acc: 0.8815 - val_loss: 0.3321 - val_acc: 0.8769\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 38s 903us/step - loss: 0.2905 - acc: 0.8927 - val_loss: 0.3248 - val_acc: 0.8779\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 38s 901us/step - loss: 0.2696 - acc: 0.9000 - val_loss: 0.3091 - val_acc: 0.8886\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 38s 894us/step - loss: 0.2524 - acc: 0.9060 - val_loss: 0.3063 - val_acc: 0.8916\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 40s 948us/step - loss: 0.2357 - acc: 0.9127 - val_loss: 0.3101 - val_acc: 0.8866\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 41s 987us/step - loss: 0.2198 - acc: 0.9178 - val_loss: 0.3181 - val_acc: 0.8870\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 40s 950us/step - loss: 0.2081 - acc: 0.9230 - val_loss: 0.2969 - val_acc: 0.8943\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 34s 805us/step - loss: 0.4778 - acc: 0.8341 - val_loss: 0.3473 - val_acc: 0.8777\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 34s 803us/step - loss: 0.3188 - acc: 0.8867 - val_loss: 0.3324 - val_acc: 0.8806\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 34s 804us/step - loss: 0.2736 - acc: 0.9017 - val_loss: 0.3066 - val_acc: 0.8900\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 34s 802us/step - loss: 0.2475 - acc: 0.9083 - val_loss: 0.2962 - val_acc: 0.8947\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 34s 813us/step - loss: 0.2244 - acc: 0.9182 - val_loss: 0.3056 - val_acc: 0.8906\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 34s 805us/step - loss: 0.2037 - acc: 0.9246 - val_loss: 0.2723 - val_acc: 0.9061\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 34s 804us/step - loss: 0.1872 - acc: 0.9309 - val_loss: 0.3015 - val_acc: 0.8985\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 34s 808us/step - loss: 0.1714 - acc: 0.9352 - val_loss: 0.2848 - val_acc: 0.9056\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 34s 802us/step - loss: 0.1547 - acc: 0.9425 - val_loss: 0.2997 - val_acc: 0.9027\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 34s 805us/step - loss: 0.1437 - acc: 0.9455 - val_loss: 0.3068 - val_acc: 0.9033\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 37s 887us/step - loss: 0.5184 - acc: 0.8105 - val_loss: 0.3901 - val_acc: 0.8589\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 38s 906us/step - loss: 0.3382 - acc: 0.8777 - val_loss: 0.3248 - val_acc: 0.8831\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 38s 916us/step - loss: 0.2956 - acc: 0.8919 - val_loss: 0.3397 - val_acc: 0.8782\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 39s 924us/step - loss: 0.2673 - acc: 0.9015 - val_loss: 0.2874 - val_acc: 0.8970\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 38s 915us/step - loss: 0.2440 - acc: 0.9106 - val_loss: 0.2860 - val_acc: 0.8974\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 38s 913us/step - loss: 0.2211 - acc: 0.9161 - val_loss: 0.2633 - val_acc: 0.9021\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 38s 913us/step - loss: 0.2042 - acc: 0.9235 - val_loss: 0.2537 - val_acc: 0.9102\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 39s 929us/step - loss: 0.1855 - acc: 0.9310 - val_loss: 0.2669 - val_acc: 0.9058\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.1696 - acc: 0.9365 - val_loss: 0.2716 - val_acc: 0.9061\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.1577 - acc: 0.9413 - val_loss: 0.2547 - val_acc: 0.9097\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 45s 1ms/step - loss: 0.6445 - acc: 0.7623 - val_loss: 0.4694 - val_acc: 0.8312\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.4328 - acc: 0.8415 - val_loss: 0.4079 - val_acc: 0.8525\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.3768 - acc: 0.8609 - val_loss: 0.3910 - val_acc: 0.8605\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.3453 - acc: 0.8722 - val_loss: 0.3575 - val_acc: 0.8687\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.3207 - acc: 0.8825 - val_loss: 0.3355 - val_acc: 0.8792\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.2995 - acc: 0.8892 - val_loss: 0.3269 - val_acc: 0.8847\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2863 - acc: 0.8938 - val_loss: 0.3297 - val_acc: 0.8811\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2703 - acc: 0.8994 - val_loss: 0.3134 - val_acc: 0.8845\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2560 - acc: 0.9031 - val_loss: 0.3154 - val_acc: 0.8863\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2459 - acc: 0.9086 - val_loss: 0.3518 - val_acc: 0.8744\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 41s 986us/step - loss: 0.4563 - acc: 0.8383 - val_loss: 0.3442 - val_acc: 0.8762\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 41s 970us/step - loss: 0.3044 - acc: 0.8901 - val_loss: 0.3043 - val_acc: 0.8882\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 41s 969us/step - loss: 0.2598 - acc: 0.9046 - val_loss: 0.2733 - val_acc: 0.9012\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 41s 973us/step - loss: 0.2294 - acc: 0.9147 - val_loss: 0.2763 - val_acc: 0.9024\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 41s 972us/step - loss: 0.2029 - acc: 0.9249 - val_loss: 0.2934 - val_acc: 0.8997\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 41s 974us/step - loss: 0.1815 - acc: 0.9329 - val_loss: 0.2730 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 41s 979us/step - loss: 0.1620 - acc: 0.9390 - val_loss: 0.2790 - val_acc: 0.9062\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 41s 975us/step - loss: 0.1445 - acc: 0.9457 - val_loss: 0.2849 - val_acc: 0.9077\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 41s 975us/step - loss: 0.1277 - acc: 0.9517 - val_loss: 0.2991 - val_acc: 0.9082\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 41s 975us/step - loss: 0.1134 - acc: 0.9573 - val_loss: 0.3196 - val_acc: 0.9060\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 39s 940us/step - loss: 0.5159 - acc: 0.8135 - val_loss: 0.3875 - val_acc: 0.8600\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 40s 950us/step - loss: 0.3349 - acc: 0.8768 - val_loss: 0.3134 - val_acc: 0.8906\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2900 - acc: 0.8925 - val_loss: 0.2898 - val_acc: 0.8944\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2567 - acc: 0.9058 - val_loss: 0.2846 - val_acc: 0.8947\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2335 - acc: 0.9115 - val_loss: 0.2547 - val_acc: 0.9077\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.2109 - acc: 0.9205 - val_loss: 0.2553 - val_acc: 0.9096\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 43s 1ms/step - loss: 0.1890 - acc: 0.9289 - val_loss: 0.2474 - val_acc: 0.9122\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.1723 - acc: 0.9355 - val_loss: 0.2569 - val_acc: 0.9097\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.1549 - acc: 0.9419 - val_loss: 0.2658 - val_acc: 0.9109\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 42s 1ms/step - loss: 0.1410 - acc: 0.9477 - val_loss: 0.2611 - val_acc: 0.9157\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 38s 904us/step - loss: 0.6246 - acc: 0.7693 - val_loss: 0.4529 - val_acc: 0.8350\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 39s 937us/step - loss: 0.4169 - acc: 0.8455 - val_loss: 0.3929 - val_acc: 0.8549\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 40s 949us/step - loss: 0.3591 - acc: 0.8669 - val_loss: 0.3690 - val_acc: 0.8616\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 40s 954us/step - loss: 0.3248 - acc: 0.8784 - val_loss: 0.3449 - val_acc: 0.8718\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 40s 949us/step - loss: 0.2996 - acc: 0.8883 - val_loss: 0.3187 - val_acc: 0.8824\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 41s 968us/step - loss: 0.2794 - acc: 0.8945 - val_loss: 0.3195 - val_acc: 0.8826\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 40s 956us/step - loss: 0.2635 - acc: 0.9015 - val_loss: 0.3147 - val_acc: 0.8874\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 41s 972us/step - loss: 0.2492 - acc: 0.9075 - val_loss: 0.2865 - val_acc: 0.8957\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 40s 959us/step - loss: 0.2357 - acc: 0.9111 - val_loss: 0.3139 - val_acc: 0.8918\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 41s 975us/step - loss: 0.2222 - acc: 0.9157 - val_loss: 0.2921 - val_acc: 0.8959\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.4349 - acc: 0.8439 - val_loss: 0.3352 - val_acc: 0.8752\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 50s 1ms/step - loss: 0.2891 - acc: 0.8930 - val_loss: 0.2975 - val_acc: 0.8918\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 53s 1ms/step - loss: 0.2440 - acc: 0.9094 - val_loss: 0.2707 - val_acc: 0.9018\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 53s 1ms/step - loss: 0.2059 - acc: 0.9234 - val_loss: 0.2557 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 54s 1ms/step - loss: 0.1806 - acc: 0.9322 - val_loss: 0.2775 - val_acc: 0.9023\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 53s 1ms/step - loss: 0.1540 - acc: 0.9432 - val_loss: 0.2812 - val_acc: 0.9041\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 54s 1ms/step - loss: 0.1343 - acc: 0.9496 - val_loss: 0.2849 - val_acc: 0.9068\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 53s 1ms/step - loss: 0.1144 - acc: 0.9575 - val_loss: 0.2909 - val_acc: 0.9098\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 54s 1ms/step - loss: 0.0962 - acc: 0.9643 - val_loss: 0.3083 - val_acc: 0.9146\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 53s 1ms/step - loss: 0.0819 - acc: 0.9698 - val_loss: 0.3363 - val_acc: 0.9099\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 44s 1ms/step - loss: 0.4764 - acc: 0.8235 - val_loss: 0.3422 - val_acc: 0.8746\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 59s 1ms/step - loss: 0.3074 - acc: 0.8858 - val_loss: 0.2980 - val_acc: 0.8892\n",
      "Epoch 3/10\n",
      "37280/42000 [=========================>....] - ETA: 7s - loss: 0.2629 - acc: 0.9029"
     ]
    }
   ],
   "source": [
    "#Trying multiple convolutional layers, and dense layers in a loop and check for the best one\n",
    "#\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout,Activation,Conv2D,MaxPooling2D,LeakyReLU,BatchNormalization\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "\n",
    "dense_layers = [0,1,2]\n",
    "layer_sizes = [32,64,128] #dense layer sizes\n",
    "conv_layers = [1,2,3]\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer,layer_size,dense_layer,int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='C:/Users/djaym7/Tensorflow/logs/{}'.format(NAME))\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(filters=64,kernel_size=2,input_shape=(28,28,1) )) \n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            for i in range(conv_layer-1):\n",
    "                model.add(Conv2D(filters=64,kernel_size=2,input_shape=(28,28,1) )) \n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            model.add(Flatten())\n",
    "            for j in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                \n",
    "            model.add(Dense(10))\n",
    "            model.add(Activation('softmax'))\n",
    "            \n",
    "                \n",
    "            model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "            model.fit(X_train,y_train,epochs=10,validation_split=0.3,callbacks=[tensorboard])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 121s 3ms/step - loss: 0.3942 - acc: 0.8613 - val_loss: 0.3200 - val_acc: 0.8847\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 123s 3ms/step - loss: 0.2608 - acc: 0.9043 - val_loss: 0.2767 - val_acc: 0.8986\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 123s 3ms/step - loss: 0.2118 - acc: 0.9209 - val_loss: 0.2861 - val_acc: 0.8998\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.1732 - acc: 0.9357 - val_loss: 0.2515 - val_acc: 0.9106\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.1415 - acc: 0.9480 - val_loss: 0.2662 - val_acc: 0.9120\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.1124 - acc: 0.9577 - val_loss: 0.3104 - val_acc: 0.9039\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.0879 - acc: 0.9679 - val_loss: 0.2978 - val_acc: 0.9111\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.0708 - acc: 0.9736 - val_loss: 0.3161 - val_acc: 0.9121\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.0555 - acc: 0.9806 - val_loss: 0.3452 - val_acc: 0.9103\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 128s 3ms/step - loss: 0.0461 - acc: 0.9840 - val_loss: 0.3843 - val_acc: 0.9114\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.4348 - acc: 0.8424 - val_loss: 0.3426 - val_acc: 0.8751\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.2927 - acc: 0.8932 - val_loss: 0.2711 - val_acc: 0.9004\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.2447 - acc: 0.9080 - val_loss: 0.2727 - val_acc: 0.8995\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.2096 - acc: 0.9221 - val_loss: 0.2524 - val_acc: 0.9099\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.1739 - acc: 0.9347 - val_loss: 0.2382 - val_acc: 0.9122\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.1462 - acc: 0.9453 - val_loss: 0.2498 - val_acc: 0.9121\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.1184 - acc: 0.9550 - val_loss: 0.2530 - val_acc: 0.9156\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.0963 - acc: 0.9636 - val_loss: 0.2845 - val_acc: 0.9152\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.0764 - acc: 0.9721 - val_loss: 0.3125 - val_acc: 0.9073\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 0.0620 - acc: 0.9768 - val_loss: 0.3326 - val_acc: 0.9108\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 135s 3ms/step - loss: 0.4136 - acc: 0.8497 - val_loss: 0.3126 - val_acc: 0.8877\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 134s 3ms/step - loss: 0.2695 - acc: 0.9001 - val_loss: 0.2725 - val_acc: 0.9009\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 134s 3ms/step - loss: 0.2154 - acc: 0.9198 - val_loss: 0.2778 - val_acc: 0.9046\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 134s 3ms/step - loss: 0.1763 - acc: 0.9332 - val_loss: 0.2592 - val_acc: 0.9106\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 134s 3ms/step - loss: 0.1417 - acc: 0.9455 - val_loss: 0.2767 - val_acc: 0.9107\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 135s 3ms/step - loss: 0.1126 - acc: 0.9580 - val_loss: 0.3031 - val_acc: 0.9062\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 135s 3ms/step - loss: 0.0904 - acc: 0.9661 - val_loss: 0.3361 - val_acc: 0.9092\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 135s 3ms/step - loss: 0.0712 - acc: 0.9748 - val_loss: 0.3911 - val_acc: 0.9048\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 135s 3ms/step - loss: 0.0629 - acc: 0.9775 - val_loss: 0.3878 - val_acc: 0.9103\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 135s 3ms/step - loss: 0.0524 - acc: 0.9808 - val_loss: 0.4504 - val_acc: 0.9074980 - ETA: 2s - \n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.4373 - acc: 0.8398 - val_loss: 0.3217 - val_acc: 0.8801\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.2837 - acc: 0.8934 - val_loss: 0.2777 - val_acc: 0.8994\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.2355 - acc: 0.9114 - val_loss: 0.2849 - val_acc: 0.8977\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.1953 - acc: 0.9256 - val_loss: 0.2687 - val_acc: 0.9036\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.1645 - acc: 0.9390 - val_loss: 0.2607 - val_acc: 0.9122\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.1361 - acc: 0.9483 - val_loss: 0.3103 - val_acc: 0.9047\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.1135 - acc: 0.9560 - val_loss: 0.2864 - val_acc: 0.9138\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.0902 - acc: 0.9655 - val_loss: 0.3101 - val_acc: 0.9158\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.0752 - acc: 0.9720 - val_loss: 0.3146 - val_acc: 0.9166\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 64s 2ms/step - loss: 0.0640 - acc: 0.9761 - val_loss: 0.3881 - val_acc: 0.9124\n"
     ]
    }
   ],
   "source": [
    "#   1-128-1 and 2-128-1 worked the best, so trying to add more layers in dense to check if it gets better eg 512 \n",
    "#\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout,Activation,Conv2D,MaxPooling2D,LeakyReLU,BatchNormalization\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "\n",
    "dense_layers = [1,2]\n",
    "layer_sizes = [512] #Getting out of memory (8gb) and kernel is dying :(\n",
    "conv_layers = [1,2]\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer,layer_size,dense_layer,int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='C:/Users/djaym7/Tensorflow/logs/{}'.format(NAME))\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(filters=64,kernel_size=2,input_shape=(28,28,1) )) \n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            for i in range(conv_layer-1):\n",
    "                model.add(Conv2D(filters=64,kernel_size=2,input_shape=(28,28,1) )) \n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            model.add(Flatten())\n",
    "            for j in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                \n",
    "            model.add(Dense(10))\n",
    "            model.add(Activation('softmax'))\n",
    "            \n",
    "                \n",
    "            model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "            model.fit(X_train,y_train,epochs=10,validation_split=0.3,callbacks=[tensorboard])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
